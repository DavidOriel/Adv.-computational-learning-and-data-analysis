{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":48610,"status":"ok","timestamp":1741589730050,"user":{"displayName":"Shachar Ashkenazi","userId":"00493665488606524942"},"user_tz":-120},"id":"wbPxf97p95Dk","outputId":"d0cc1ddd-63a6-404d-fd46-e47d97f193c7"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import pandas as pd\n","import numpy as np\n","import os\n","import shutil\n","from sklearn.preprocessing import MultiLabelBinarizer\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms,models\n","from sklearn.model_selection import train_test_split\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from PIL import Image\n","import ast\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ssPfgi4f9-2k"},"outputs":[],"source":["one_hot_train_df = pd.read_csv('/content/drive/MyDrive/Deep Final Work/Training dfs/one_hot_train_df.csv')\n","one_hot_test_df = pd.read_csv('/content/drive/MyDrive/Deep Final Work/Training dfs/one_hot_test_df.csv')\n","\n","index_train_df = pd.read_csv('/content/drive/MyDrive/Deep Final Work/Training dfs/index_train_df.csv')\n","index_test_df = pd.read_csv('/content/drive/MyDrive/Deep Final Work/Training dfs/index_test_df.csv')\n","\n","w2v_train_df = pd.read_csv('/content/drive/MyDrive/Deep Final Work/Training dfs/w2v_train_df.csv')\n","w2v_test_df = pd.read_csv('/content/drive/MyDrive/Deep Final Work/Training dfs/w2v_test_df.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j9M789BeSkjd"},"outputs":[],"source":["transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),  # Standard ImageNet normalization\n","])"]},{"cell_type":"markdown","metadata":{"id":"_WZTWc63-VbB"},"source":["#Train and evaluate resnet over one-hot"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QLSW7MDP-Fhc"},"outputs":[],"source":["class CalorieOneHotDataset(Dataset):\n","    def __init__(self, df, calorie_col='dish_calories', transform=None):\n","\n","        self.df = df.reset_index(drop=True)\n","        self.transform = transform\n","        self.calorie_col = calorie_col\n","\n","        self.ingredient_cols = [col for col in df.columns if col not in ['path', calorie_col]]\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, idx):\n","        row = self.df.iloc[idx]\n","        image_path = row['path']\n","\n","        image = Image.open(image_path).convert('RGB')\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        # Get one-hot encoded ingredient vector as a tensor\n","        ingredients_vector = row[self.ingredient_cols].values.astype(np.float32)\n","        ingredients_vector = torch.tensor(ingredients_vector, dtype=torch.float)\n","\n","        # Get calorie target\n","        calories = torch.tensor(row[self.calorie_col], dtype=torch.float)\n","\n","        return image, ingredients_vector, calories\n","\n","train_dataset_onehot = CalorieOneHotDataset(one_hot_train_df, transform=transform)\n","test_dataset_onehot = CalorieOneHotDataset(one_hot_test_df, transform=transform)\n","\n","train_loader_onehot = DataLoader(train_dataset_onehot, batch_size=8, shuffle=True)\n","test_loader_onehot = DataLoader(test_dataset_onehot, batch_size=8, shuffle=False)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SHM4L4X2Um6q"},"outputs":[],"source":["class CaloriePredictorOneHot(nn.Module):\n","    def __init__(self, ingredient_dim, hidden_dim=128):\n","\n","        super().__init__()\n","        # Load a pretrained ResNet-18 and remove its classification head.\n","        self.resnet = models.resnet18(pretrained=True)\n","        num_features = self.resnet.fc.in_features\n","        self.resnet.fc = nn.Identity()\n","\n","        # Process the one-hot ingredient vector.\n","        self.ingredient_fc = nn.Sequential(\n","            nn.Linear(ingredient_dim, 64),\n","            nn.ReLU(),\n","            nn.Linear(64, 32),\n","            nn.ReLU()\n","        )\n","        combined_dim = num_features + 32\n","        self.fc = nn.Sequential(\n","            nn.Linear(combined_dim, hidden_dim),\n","            nn.ReLU(),\n","            nn.Linear(hidden_dim, 1)\n","        )\n","\n","    def forward(self, image, ingredient_vector):\n","        img_feat = self.resnet(image)\n","        ingr_feat = self.ingredient_fc(ingredient_vector)\n","        combined = torch.cat([img_feat, ingr_feat], dim=1)\n","        out = self.fc(combined)\n","        return out.squeeze(1)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"7B1vyyCVBajE","outputId":"d322562e-c533-4523-8c39-5295342d1d85"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n","100%|██████████| 44.7M/44.7M [00:00<00:00, 104MB/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/20 | Train Loss: 65611.3316 | Test Loss: 17340.3350\n","Epoch 2/20 | Train Loss: 21335.1766 | Test Loss: 7255.3729\n","Epoch 3/20 | Train Loss: 15157.8153 | Test Loss: 10798.5745\n","Epoch 4/20 | Train Loss: 12617.5136 | Test Loss: 9053.1739\n","Epoch 5/20 | Train Loss: 10070.1696 | Test Loss: 5600.4849\n","Epoch 6/20 | Train Loss: 8705.5140 | Test Loss: 5883.8551\n","Epoch 7/20 | Train Loss: 6925.2571 | Test Loss: 7002.7781\n","Epoch 8/20 | Train Loss: 6198.0118 | Test Loss: 5439.5540\n","Epoch 9/20 | Train Loss: 5468.4597 | Test Loss: 5520.9571\n","Epoch 10/20 | Train Loss: 4843.2717 | Test Loss: 4872.8347\n","Epoch 11/20 | Train Loss: 4539.3202 | Test Loss: 5751.6928\n","Epoch 12/20 | Train Loss: 4284.4866 | Test Loss: 4422.0901\n","Epoch 13/20 | Train Loss: 4072.8774 | Test Loss: 5254.1906\n","Epoch 14/20 | Train Loss: 4187.6054 | Test Loss: 6938.7376\n","Epoch 15/20 | Train Loss: 3849.8642 | Test Loss: 5493.1738\n","Epoch 16/20 | Train Loss: 3575.3023 | Test Loss: 5158.2928\n","Epoch 17/20 | Train Loss: 3470.4702 | Test Loss: 4723.6907\n","Epoch 18/20 | Train Loss: 3189.9187 | Test Loss: 7326.5488\n","Epoch 19/20 | Train Loss: 3123.5823 | Test Loss: 4818.5958\n","Epoch 20/20 | Train Loss: 2672.8597 | Test Loss: 6292.8573\n","✅ Model training complete and saved!\n"]}],"source":["\n","# -------------------------------\n","# Training and Evaluation Code\n","# -------------------------------\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","ingredient_dim = len(train_dataset_onehot.ingredient_cols)\n","\n","onehot_model = CaloriePredictorOneHot(ingredient_dim=ingredient_dim).to(device)\n","\n","criterion = nn.MSELoss()  # Mean Squared Error for regression.\n","optimizer = optim.Adam(onehot_model.parameters(), lr=1e-4)\n","\n","num_epochs = 20\n","\n","for epoch in range(num_epochs):\n","    # Training phase.\n","    onehot_model.train()\n","    total_train_loss = 0.0\n","    for images, ingredients_vector, calories in train_loader_onehot:\n","        images = images.to(device)\n","        ingredients_vector = ingredients_vector.to(device)\n","        calories = calories.to(device)\n","\n","        optimizer.zero_grad()\n","        outputs = onehot_model(images, ingredients_vector)\n","        loss = criterion(outputs, calories)\n","        loss.backward()\n","        optimizer.step()\n","\n","        total_train_loss += loss.item() * images.size(0)\n","    avg_train_loss = total_train_loss / len(train_dataset_onehot)\n","\n","    # Evaluation phase.\n","    onehot_model.eval()\n","    total_test_loss = 0.0\n","    with torch.no_grad():\n","        for images, ingredients_vector, calories in test_loader_onehot:\n","            images = images.to(device)\n","            ingredients_vector = ingredients_vector.to(device)\n","            calories = calories.to(device)\n","\n","            outputs = onehot_model(images, ingredients_vector)\n","            loss = criterion(outputs, calories)\n","            total_test_loss += loss.item() * images.size(0)\n","    avg_test_loss = total_test_loss / len(test_dataset_onehot)\n","\n","    print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {avg_train_loss:.4f} | Test Loss: {avg_test_loss:.4f}\")\n","\n","# Save the trained model.\n","torch.save(onehot_model,\"/content/drive/MyDrive/Deep Final Work/Saved Models/calorie_predictor_resnet_onehot1.pth\")\n","print(\"✅ Model training complete and saved!\")"]},{"cell_type":"markdown","metadata":{"id":"QMQIKIVdCcKy"},"source":["# Train and evaluate resnet over index Embedding"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":410,"status":"ok","timestamp":1741527652917,"user":{"displayName":"Shachar Ashkenazi","userId":"00493665488606524942"},"user_tz":-120},"id":"qBv99Z3SDz2F","outputId":"21c2a393-f0dd-49d7-9005-fc9a365a65f2"},"outputs":[{"name":"stdout","output_type":"stream","text":["{'almonds': 1, 'apple': 2, 'artichokes': 3, 'arugula': 4, 'asparagus': 5, 'avocado': 6, 'baby carrots': 7, 'bacon': 8, 'bagels': 9, 'banana with peel': 10, 'basil': 11, 'beef': 12, 'beets': 13, 'bell peppers': 14, 'berries': 15, 'black beans': 16, 'blackberries': 17, 'blue cheese': 18, 'blueberries': 19, 'bok choy': 20, 'bread': 21, 'broccoli': 22, 'brown rice': 23, 'brown sugar': 24, 'brownies': 25, 'brussels sprouts': 26, 'bulgur': 27, 'butter': 28, 'buttermilk': 29, 'cabbage': 30, 'caesar dressing': 31, 'caesar salad': 32, 'cantaloupe': 33, 'carrot': 34, 'cauliflower': 35, 'celery': 36, 'celery root': 37, 'cereal': 38, 'chard': 39, 'chayote squash': 40, 'cheese': 41, 'cheese pizza': 42, 'cherry tomatoes': 43, 'chia seeds': 44, 'chicken': 45, 'chicken apple sausage': 46, 'chicken breast': 47, 'chicken salad': 48, 'chicken thighs': 49, 'chickpeas': 50, 'chilaquiles': 51, 'chili': 52, 'chive': 53, 'cilantro': 54, 'cod': 55, 'cookies': 56, 'corn': 57, 'corn on the cob': 58, 'cottage cheese': 59, 'country rice': 60, 'couscous': 61, 'cranberries': 62, 'cream': 63, 'cream cheese': 64, 'croutons': 65, 'cucumbers': 66, 'deprecated': 67, 'egg whites': 68, 'eggplant': 69, 'eggs': 70, 'feta cheese': 71, 'figs': 72, 'fish': 73, 'flour': 74, 'fried rice': 75, 'frozen yogurt': 76, 'fruit salad': 77, 'garden salad': 78, 'garlic': 79, 'ginger': 80, 'goat cheese': 81, 'granola': 82, 'grapefruit juice': 83, 'grapes': 84, 'greek salad': 85, 'greek yogurt': 86, 'green beans': 87, 'green onions': 88, 'grilled chicken': 89, 'ground turkey': 90, 'hash browns': 91, 'hominy': 92, 'honeydew melons': 93, 'ice cream cones': 94, 'jalapenos': 95, 'jicama': 96, 'kale': 97, 'ketchup': 98, 'kimchi': 99, 'lemon juice': 100, 'lentils': 101, 'lettuce': 102, 'lime': 103, 'mandarin oranges': 104, 'mangos': 105, 'mayonnaise': 106, 'milk': 107, 'millet': 108, 'mixed greens': 109, 'mozzarella cheese': 110, 'mushroom': 111, 'mustard': 112, 'mustard greens': 113, 'nopales': 114, 'oatmeal': 115, 'olive oil': 116, 'olives': 117, 'onions': 118, 'orange': 119, 'orange juice': 120, 'orange with peel': 121, 'oregano': 122, 'parmesan cheese': 123, 'parsley': 124, 'parsnips': 125, 'pasta': 126, 'pasta salad': 127, 'pears': 128, 'pecans': 129, 'pepper': 130, 'pepperoni': 131, 'pepperoni pizza': 132, 'pesto': 133, 'pickles': 134, 'pilaf': 135, 'pineapple': 136, 'pizza': 137, 'plate only': 138, 'pork': 139, 'potatoes': 140, 'pumpkin seeds': 141, 'quinoa': 142, 'radishes': 143, 'raisins': 144, 'raspberries': 145, 'rice noodles': 146, 'roasted potatoes': 147, 'rosemary': 148, 'salmon': 149, 'salsa': 150, 'salt': 151, 'sandwiches': 152, 'sausage': 153, 'scrambled eggs': 154, 'shallots': 155, 'snow peas': 156, 'sour cream': 157, 'soy sauce': 158, 'spinach (cooked)': 159, 'spinach (raw)': 160, 'squash': 161, 'steak': 162, 'strawberries': 163, 'succotash': 164, 'sugar': 165, 'sun dried tomatoes': 166, 'sunflower seeds': 167, 'sweet potato': 168, 'syrup': 169, 'tatsoi': 170, 'tempeh': 171, 'thyme': 172, 'toast': 173, 'tofu': 174, 'tomatillo': 175, 'tomatoes': 176, 'tortilla': 177, 'tortilla chips': 178, 'tuna': 179, 'tuna salad': 180, 'turkey bacon': 181, 'turnips': 182, 'vegetable oil': 183, 'vinaigrette': 184, 'vinegar': 185, 'waffles': 186, 'walnuts': 187, 'watermelon': 188, 'wheat berry': 189, 'wheat bread': 190, 'white beans': 191, 'white rice': 192, 'white wine': 193, 'wild rice': 194, 'wine': 195, 'yam': 196, 'yogurt': 197, 'zucchini': 198}\n"]}],"source":["import pickle\n","\n","# Load dictionary from pickle file\n","with open(\"/content/drive/MyDrive/Deep Final Work/Training dfs/ingredient_to_index.pkl\", \"rb\") as f:\n","    ingredient_to_index = pickle.load(f)\n","\n","print(ingredient_to_index)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MMy62CmsCiKC"},"outputs":[],"source":["class CalorieIndexDataset(Dataset):\n","    def __init__(self, df, transform=None, calorie_col='dish_calories', ingredient_col='ingredients_embedding'):\n","        \"\"\"\n","        Args:\n","            df (pd.DataFrame): DataFrame with columns:\n","                - 'path': image file path.\n","                - ingredient_col: padded list of ingredient indices (or its string representation).\n","                - calorie_col: dish calorie value.\n","            transform: Image transformations.\n","        \"\"\"\n","        self.df = df.reset_index(drop=True)\n","        self.transform = transform\n","        self.calorie_col = calorie_col\n","        self.ingredient_col = ingredient_col\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, idx):\n","        row = self.df.iloc[idx]\n","        image = Image.open(row['path']).convert('RGB')\n","        if self.transform:\n","            image = self.transform(image)\n","        ingredient_data = row[self.ingredient_col]\n","        # If ingredient_data is a string, convert it to a list.\n","        if isinstance(ingredient_data, str):\n","            ingredient_data = ast.literal_eval(ingredient_data)\n","        ingredient_indices = torch.tensor(ingredient_data, dtype=torch.long)\n","        # Get calorie target.\n","        calories = torch.tensor(row[self.calorie_col], dtype=torch.float)\n","        return image, ingredient_indices, calories\n","\n","\n","train_dataset_index = CalorieIndexDataset(index_train_df, transform=transform)\n","test_dataset_index  = CalorieIndexDataset(index_test_df, transform=transform)\n","\n","train_loader_index = DataLoader(train_dataset_index, batch_size=8, shuffle=True)\n","test_loader_index  = DataLoader(test_dataset_index, batch_size=8, shuffle=False)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4s76lPSRLN9S"},"outputs":[],"source":["class CaloriePredictorIndices(nn.Module):\n","    def __init__(self, vocab_size, embed_dim=50, hidden_dim=128):\n","        \"\"\"\n","        Args:\n","            vocab_size (int): Number of unique ingredients.\n","            embed_dim (int): Embedding dimension.\n","            hidden_dim (int): Hidden units in the FC layer after concatenation.\n","        \"\"\"\n","        super().__init__()\n","        self.resnet = models.resnet18(pretrained=True)\n","        num_features = self.resnet.fc.in_features\n","        self.resnet.fc = nn.Identity()\n","        self.embedding = nn.Embedding(num_embeddings=vocab_size + 1, embedding_dim=embed_dim, padding_idx=0)\n","        combined_dim = num_features + embed_dim\n","        self.fc = nn.Sequential(\n","            nn.Linear(combined_dim, hidden_dim),\n","            nn.ReLU(),\n","            nn.Linear(hidden_dim, 1)  # Regression output for calories.\n","        )\n","\n","    def forward(self, image, ingredient_indices):\n","        image_features = self.resnet(image)\n","        ingr_embeds = self.embedding(ingredient_indices)\n","        ingredient_features = ingr_embeds.mean(dim=1)\n","        combined = torch.cat([image_features, ingredient_features], dim=1)\n","        out = self.fc(combined)\n","        return out.squeeze(1)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"75X8HpS9IIhO","outputId":"823fdb70-a03b-46fa-99d1-f287c98e210d"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/10 | Train Loss: 65632.9159 | Test Loss: 33841.0128\n","Epoch 2/10 | Train Loss: 22454.1116 | Test Loss: 7714.6401\n","Epoch 3/10 | Train Loss: 15666.6690 | Test Loss: 8057.5698\n","Epoch 4/10 | Train Loss: 13560.1747 | Test Loss: 14052.9516\n","Epoch 5/10 | Train Loss: 10205.4769 | Test Loss: 8512.7222\n","Epoch 6/10 | Train Loss: 8162.1674 | Test Loss: 10292.6881\n","Epoch 7/10 | Train Loss: 7060.4899 | Test Loss: 9101.5987\n","Epoch 8/10 | Train Loss: 5511.3396 | Test Loss: 7238.1546\n","Epoch 9/10 | Train Loss: 5886.6755 | Test Loss: 8765.4848\n","Epoch 10/10 | Train Loss: 5035.7313 | Test Loss: 9943.3935\n","✅ Model training complete and saved!\n"]}],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","vocab_size = len(ingredient_to_index)\n","index_model = CaloriePredictorIndices(vocab_size=vocab_size, embed_dim=50, hidden_dim=128)\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","index_model = index_model.to(device)\n","\n","criterion = nn.MSELoss()\n","optimizer = optim.Adam(index_model.parameters(), lr=1e-4)\n","num_epochs = 10\n","\n","for epoch in range(num_epochs):\n","    index_model.train()\n","    total_train_loss = 0.0\n","    for images, ingredient_indices, calories in train_loader_index:\n","        images = images.to(device)\n","        ingredient_indices = ingredient_indices.to(device)\n","        calories = calories.to(device)\n","\n","        optimizer.zero_grad()\n","        outputs = index_model(images, ingredient_indices)\n","        loss = criterion(outputs, calories)\n","        loss.backward()\n","        optimizer.step()\n","\n","        total_train_loss += loss.item() * images.size(0)\n","    avg_train_loss = total_train_loss / len(train_dataset_index)\n","\n","    index_model.eval()\n","    total_test_loss = 0.0\n","    with torch.no_grad():\n","        for images, ingredient_indices, calories in test_loader_index:\n","            images = images.to(device)\n","            ingredient_indices = ingredient_indices.to(device)\n","            calories = calories.to(device)\n","\n","            outputs = index_model(images, ingredient_indices)\n","            loss = criterion(outputs, calories)\n","            total_test_loss += loss.item() * images.size(0)\n","    avg_test_loss = total_test_loss / len(test_dataset_index)\n","\n","    print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {avg_train_loss:.4f} | Test Loss: {avg_test_loss:.4f}\")\n","\n","# Save the trained model.\n","torch.save(index_model, \"/content/drive/MyDrive/Deep Final Work/Saved Models/calorie_predictor_resnet_indices.pth\")\n","print(\"✅ Model training complete and saved!\")"]},{"cell_type":"markdown","metadata":{"id":"k4M9AaCSObOB"},"source":["Training and evaluating word2vec model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5GoxWSGlOfxF"},"outputs":[],"source":["class CalorieDatasetWithEmbeddings(Dataset):\n","    def __init__(self, df, transform=None, calorie_col='dish_calories'):\n","        \"\"\"\n","        Args:\n","            df (pd.DataFrame): DataFrame with 'path', 'ingredient_embedding', and calorie info.\n","            transform: Image transformations to apply.\n","            calorie_col (str): Column name for the calorie target.\n","        \"\"\"\n","        self.df = df.reset_index(drop=True)\n","        self.transform = transform\n","        self.calorie_col = calorie_col\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, idx):\n","        row = self.df.iloc[idx]\n","        image = Image.open(row['path']).convert('RGB')\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        embedding_data = row['ingredient_embedding']\n","        if isinstance(embedding_data, str):\n","            # If the string contains commas, use literal_eval\n","            if ',' in embedding_data:\n","                embedding_data = ast.literal_eval(embedding_data)\n","            else:\n","                # Otherwise, assume it's space-separated.\n","                embedding_data = embedding_data.strip(\"[]\")\n","                # Split on whitespace and convert each element to float.\n","                embedding_data = [float(x) for x in embedding_data.split()]\n","        ingredient_embedding = torch.tensor(embedding_data, dtype=torch.float)\n","\n","        calories = torch.tensor(row[self.calorie_col], dtype=torch.float)\n","\n","        return image, ingredient_embedding, calories\n","\n","\n","train_dataset_w2v = CalorieDatasetWithEmbeddings(w2v_train_df, transform=transform)\n","test_dataset_w2v  = CalorieDatasetWithEmbeddings(w2v_test_df, transform=transform)\n","\n","train_loader_w2v = DataLoader(train_dataset_w2v, batch_size=8, shuffle=True)\n","test_loader_w2v  = DataLoader(test_dataset_w2v, batch_size=8, shuffle=False)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Hos7ThouQMpO"},"outputs":[],"source":["class CaloriePredictorW2V(nn.Module):\n","    def __init__(self, input_embed_dim=50, hidden_dim=128):\n","        \"\"\"\n","        Args:\n","            input_embed_dim (int): Dimension of the precomputed ingredient embedding.\n","            hidden_dim (int): Number of hidden units in the FC regressor.\n","        \"\"\"\n","        super().__init__()\n","        self.resnet = models.resnet18(pretrained=True)\n","        num_img_features = self.resnet.fc.in_features\n","        self.resnet.fc = nn.Identity()\n","        combined_dim = num_img_features + input_embed_dim\n","        self.fc = nn.Sequential(\n","            nn.Linear(combined_dim, hidden_dim),\n","            nn.ReLU(),\n","            nn.Linear(hidden_dim, 1)\n","        )\n","\n","    def forward(self, image, ingredient_embedding):\n","        image_features = self.resnet(image)\n","        combined = torch.cat([image_features, ingredient_embedding], dim=1)\n","        out = self.fc(combined)\n","        return out.squeeze(1)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"EdEfWdo6QQgg","outputId":"f2342a29-1a4a-44dc-b7c3-38b3ceedf34b"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/20 | Train Loss: 68311.9144 | Test Loss: 17228.9120\n","Epoch 2/20 | Train Loss: 23529.6648 | Test Loss: 9601.0068\n","Epoch 3/20 | Train Loss: 16174.5292 | Test Loss: 6315.2299\n","Epoch 4/20 | Train Loss: 13891.9486 | Test Loss: 7445.8075\n","Epoch 5/20 | Train Loss: 11062.4659 | Test Loss: 6077.9290\n","Epoch 6/20 | Train Loss: 8528.8354 | Test Loss: 6233.7331\n","Epoch 7/20 | Train Loss: 6702.3585 | Test Loss: 7106.9502\n","Epoch 8/20 | Train Loss: 6025.6422 | Test Loss: 5890.2245\n","Epoch 9/20 | Train Loss: 5131.3018 | Test Loss: 5549.0421\n","Epoch 10/20 | Train Loss: 4978.7654 | Test Loss: 8483.1475\n","Epoch 11/20 | Train Loss: 4150.4109 | Test Loss: 7399.6059\n","Epoch 12/20 | Train Loss: 3994.0604 | Test Loss: 4903.1848\n","Epoch 13/20 | Train Loss: 3731.2161 | Test Loss: 5010.8068\n","Epoch 14/20 | Train Loss: 3409.6811 | Test Loss: 5393.4058\n","Epoch 15/20 | Train Loss: 3128.4786 | Test Loss: 7174.9706\n","Epoch 16/20 | Train Loss: 3081.4382 | Test Loss: 7377.7380\n","Epoch 17/20 | Train Loss: 3139.4959 | Test Loss: 7919.8309\n","Epoch 18/20 | Train Loss: 2826.6835 | Test Loss: 6030.1119\n","Epoch 19/20 | Train Loss: 3810.9566 | Test Loss: 6271.3784\n","Epoch 20/20 | Train Loss: 2868.0931 | Test Loss: 5667.3957\n","✅ Model training complete and saved!\n"]}],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","w2v_model = CaloriePredictorW2V(input_embed_dim=50, hidden_dim=128).to(device)\n","\n","criterion = nn.MSELoss()  # Mean Squared Error for regression.\n","optimizer = optim.Adam(w2v_model.parameters(), lr=1e-4)\n","num_epochs = 20\n","\n","for epoch in range(num_epochs):\n","    w2v_model.train()\n","    total_train_loss = 0.0\n","    for images, ingredient_embedding, calories in train_loader_w2v:\n","        images = images.to(device)\n","        ingredient_embedding = ingredient_embedding.to(device)\n","        calories = calories.to(device)\n","\n","        optimizer.zero_grad()\n","        outputs = w2v_model(images, ingredient_embedding)\n","        loss = criterion(outputs, calories)\n","        loss.backward()\n","        optimizer.step()\n","\n","        total_train_loss += loss.item() * images.size(0)\n","    avg_train_loss = total_train_loss / len(train_dataset_w2v)\n","\n","    w2v_model.eval()\n","    total_test_loss = 0.0\n","    with torch.no_grad():\n","        for images, ingredient_embedding, calories in test_loader_w2v:\n","            images = images.to(device)\n","            ingredient_embedding = ingredient_embedding.to(device)\n","            calories = calories.to(device)\n","\n","            outputs = w2v_model(images, ingredient_embedding)\n","            loss = criterion(outputs, calories)\n","            total_test_loss += loss.item() * images.size(0)\n","    avg_test_loss = total_test_loss / len(test_dataset_w2v)\n","\n","    print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {avg_train_loss:.4f} | Test Loss: {avg_test_loss:.4f}\")\n","\n","# Save the trained model.\n","torch.save(w2v_model, \"/content/drive/MyDrive/Deep Final Work/Saved Models/calorie_predictor_resnet_word2vec1.pth\")\n","print(\"✅ Model training complete and saved!\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j8EHjKpbFhrw"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}